#include <cuda.h>
#include <iostream>

#define BLOCK_DIM 1024

// This is the code from the book but I couldn't get this to run faster even
// with occupancy calculator L1 throughput is dramatically increased though
__global__ void SharedMemoryReduction(float *input, float *output) {
  __shared__ float input_s[BLOCK_DIM];
  unsigned int t = threadIdx.x;
  input_s[t] = input[t] + input[t + BLOCK_DIM];
  for (unsigned int stride = blockDim.x / 2; stride >= 1; stride /= 2) {
    __syncthreads();
    if (threadIdx.x < stride) {
      input_s[t] += input_s[t + stride];
    }
  }

  if (threadIdx.x == 0) {
    *output = input_s[0];
  }
}

int main() {
  // Size of the input data
  const int size = 2048;
  const int bytes = size * sizeof(float);

  // Allocate memory for input and output on host
  float *h_input = new float[size];
  float *h_output = new float;

  // Initialize input data on host
  for (int i = 0; i < size; i++) {
    h_input[i] = 1.0f; // Example: Initialize all elements to 1
  }

  // Allocate memory for input and output on device
  float *d_input;
  float *d_output;
  cudaMalloc(&d_input, bytes);
  cudaMalloc(&d_output, sizeof(float));

  // Copy data from host to device
  cudaMemcpy(d_input, h_input, bytes, cudaMemcpyHostToDevice);

  // Launch the kernel
  SharedMemoryReduction<<<1, size / 2>>>(d_input, d_output);

  // Copy result back to host
  cudaMemcpy(h_output, d_output, sizeof(float), cudaMemcpyDeviceToHost);

  // Print the result
  std::cout << "Sum is " << *h_output << std::endl;

  // Cleanup
  delete[] h_input;
  delete h_output;
  cudaFree(d_input);
  cudaFree(d_output);

  return 0;
}
